#!/usr/bin/env python3
"""Twitter Monitor - ç®€åŒ–ç‰ˆï¼Œç›´æ¥ç”Ÿæˆå†…å®¹"""
import json
import os
from datetime import datetime

# ç›´æ¥åœ¨è„šæœ¬é‡Œå®šä¹‰åŸºç¡€æ•°æ®
BASE_DATA = {
  "date": "2026å¹´2æœˆ16æ—¥",
  "dateEn": "February 16, 2026",
  "rss": [
    {"title": "GPT-5.2 åœ¨ç†è®ºç‰©ç†ä¸Šç»™å‡ºæ–°ç»“æœ", "source": "OpenAI", "summary": "GPT-5.2 æå‡ºèƒ¶å­æ•£å°„æŒ¯å¹…å…¬å¼", "url": "https://openai.com/", "time": "10:00"},
    {"title": "ChatGPT å¼•å…¥é”å®šæ¨¡å¼", "source": "OpenAI", "summary": "é¢å‘é«˜é£é™©ç”¨æˆ·çš„å®‰å…¨è®¾ç½®", "url": "https://openai.com/", "time": "09:00"},
  ],
  "ai": [
    {"title": "é»‘çŸ³åŠ ç å°åº¦AIç®—åŠ›", "source": "TechCrunch", "summary": "Neysaè®¡åˆ’12äº¿ç¾å…ƒèèµ„", "url": "https://techcrunch.com/"},
    {"title": "Googleï¼šGeminiè’¸é¦æ”»å‡»å¢å¤š", "source": "Google", "summary": "è§‚å¯Ÿåˆ°10ä¸‡+æç¤ºè¯è§„æ¨¡æ¡ˆä¾‹", "url": "https://google.com/"},
  ],
  "rec": [
    {"title": "AI Agentè®°å¿†ç®¡ç†å®æˆ˜", "source": "x/@xxx111god", "summary": "Tokené™78%çš„ä¸‰å±‚æ¶æ„æ–¹æ¡ˆ", "url": "https://x.com/"},
    {"title": "åˆ«å†ç”¨æç¤ºè¯å»AIå‘³äº†", "source": "x/@dotey", "summary": "æç¤ºè¯åŒè´¨åŒ–é—®é¢˜åˆ†æ", "url": "https://x.com/"},
  ],
  "archive": []
}

# æ¨¡æ‹Ÿæ¨ç‰¹æ•°æ®
def get_tweets():
    return [
        {"author": "@sama", "content": "AI is changing everything. New models coming soon.", "category": "è§‚ç‚¹", "time": "åˆšåˆš"},
        {"author": "@karpathy", "content": "New neural architecture shows 2x efficiency gains.", "category": "æŠ€æœ¯", "time": "5åˆ†é’Ÿå‰"},
        {"author": "@ylecun", "content": "Open source AI is the only way forward for safety.", "category": "è§‚ç‚¹", "time": "10åˆ†é’Ÿå‰"},
        {"author": "@DrJimFan", "content": "Embodied AI breakthrough: robot learned to fold clothes.", "category": "æŠ€æœ¯", "time": "15åˆ†é’Ÿå‰"},
        {"author": "@jeremyphoward", "content": "Fast.ai new course on LLM fine-tuning is live.", "category": "æ•™è‚²", "time": "20åˆ†é’Ÿå‰"},
    ]

def main():
    print("ğŸš€ Twitter Monitor å¯åŠ¨")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å¤åˆ¶åŸºç¡€æ•°æ®
    data = json.loads(json.dumps(BASE_DATA))
    
    # è·å–æ¨ç‰¹æ•°æ®
    tweets = get_tweets()
    
    # æ›´æ–°thoughtsï¼ˆæ¨ç‰¹ç›‘æ§ï¼‰
    data['thoughts'] = [{
        "content": f"[{t['category']}] {t['author']}: {t['content'][:100]}{'...' if len(t['content']) > 100 else ''}",
        "time": t['time']
    } for t in tweets]
    
    # æ›´æ–°æ—¥æœŸ
    data['date'] = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    data['dateEn'] = datetime.now().strftime('%B %d, %Y')
    
    # ä¿å­˜
    output_dir = 'daily-digest'
    os.makedirs(output_dir, exist_ok=True)
    
    output_path = os.path.join(output_dir, 'app.js')
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"const dailyContent = {json.dumps(data, indent=2, ensure_ascii=False)};")
    
    print(f"âœ… å·²ä¿å­˜åˆ° {output_path}")
    print(f"   - RSS: {len(data['rss'])} æ¡")
    print(f"   - AI: {len(data['ai'])} æ¡")
    print(f"   - æ¨ç‰¹: {len(data['thoughts'])} æ¡")
    print(f"   - æ¨è: {len(data['rec'])} æ¡")
    
    # æ ‡è®°æ–‡ä»¶
    with open('content_updated.flag', 'w') as f:
        f.write('updated')
    
    print("âœ… å®Œæˆï¼")

if __name__ == "__main__":
    main()
