#!/usr/bin/env python3
"""
Twitter Monitor - æŠ“å–Nitter RSSå¹¶AIè¯„åˆ†ç”Ÿæˆæ‘˜è¦
æ¯5åˆ†é’Ÿè¿è¡Œï¼Œå–Top10æ¨æ–‡æ›´æ–°åˆ°ä¸»ç½‘ç«™
ä¿å­˜å†å²è®°å½•ï¼Œæ¯å¤©æœ€å¤š30æ¡
"""
import feedparser
import requests
import json
import os
import re
import time
from datetime import datetime, timedelta

# å†…å®¹å·¥å‚v2.2çš„30ä¸ªæ ¸å¿ƒTwitterè´¦å·
TWITTER_ACCOUNTS = [
    "OpenAI", "AnthropicAI", "DeepSeekAI", "GoogleAI", "MetaAI",
    "sama", "karpathy", "ylecun", "DrJimFan", "jeremyphoward",
    "goodside", "hardmaru", "gwern", "EMostaque",
    "swyx", "naval", "paulg", "elonmusk", "balajis",
    "dbakardjieva", "vincentweisser", "LinusEkenstam",
    "nmwl", "BaphometAI", "AiBreakfast", "heybingo",
    "rowancheung", "venturetwins"
]

NITTER_INSTANCES = [
    "https://nitter.net",
    "https://nitter.it", 
    "https://nitter.cz"
]

DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
HISTORY_FILE = "data/twitter_history.json"
MAX_DAILY = 30  # æ¯å¤©æœ€å¤šä¿å­˜30æ¡

def load_history():
    """åŠ è½½å†å²è®°å½•"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}

def save_history(history):
    """ä¿å­˜å†å²è®°å½•"""
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, indent=2, ensure_ascii=False)

def fetch_tweets(username, instance_idx=0):
    """ä»NitteræŠ“å–æœ€è¿‘æ¨æ–‡"""
    instance = NITTER_INSTANCES[instance_idx % len(NITTER_INSTANCES)]
    url = f"{instance}/{username}/rss"
    
    try:
        feed = feedparser.parse(url)
        tweets = []
        cutoff_time = datetime.now() - timedelta(hours=2)
        
        for entry in feed.entries[:3]:
            try:
                published = datetime(*entry.published_parsed[:6])
                if published < cutoff_time:
                    continue
                
                tweets.append({
                    "author": f"@{username}",
                    "author_name": entry.get('author', username),
                    "content": clean_tweet_text(entry.title),
                    "link": entry.link.replace("nitter.net", "x.com").replace("nitter.it", "x.com").replace("nitter.cz", "x.com"),
                    "time": published.strftime("%H:%M"),
                    "published": published.isoformat(),
                    "fetched_at": datetime.now().isoformat()
                })
            except:
                continue
        return tweets
    except Exception as e:
        print(f"âŒ Error fetching @{username}: {e}")
        return []

def clean_tweet_text(text):
    """æ¸…ç†æ¨æ–‡æ–‡æœ¬"""
    text = re.sub(r'^RT\s+@\w+:\s*', '', text)
    text = ' '.join(text.split())
    return text.strip()

def ai_score_and_summarize(tweets):
    """AIè¯„åˆ†å’Œæ‘˜è¦"""
    if not tweets or not DEEPSEEK_API_KEY:
        for t in tweets:
            t['score'] = 50
            t['summary'] = t['content'][:80] + "..." if len(t['content']) > 80 else t['content']
            t['category'] = "å…¶ä»–"
        return tweets
    
    for tweet in tweets:
        try:
            prompt = f"""åˆ†ææ¨æ–‡ï¼Œè¿”å›JSONï¼š
{{"score": 0-100æ•´æ•°, "summary": "20å­—å†…ä¸­æ–‡æ‘˜è¦", "category": "æŠ€æœ¯/äº§å“/è§‚ç‚¹/æ–°é—»/å…¶ä»–"}}

æ¨æ–‡ï¼š{tweet['content'][:800]}

åªè¿”å›JSONã€‚"""

            response = requests.post(
                "https://api.deepseek.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "deepseek-chat",
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.3,
                    "max_tokens": 200
                },
                timeout=30
            )
            
            content = response.json()['choices'][0]['message']['content']
            json_match = re.search(r'\{[^}]+\}', content)
            if json_match:
                analysis = json.loads(json_match.group())
                tweet['score'] = min(100, max(0, int(analysis.get('score', 50))))
                tweet['summary'] = analysis.get('summary', tweet['content'][:100])
                tweet['category'] = analysis.get('category', 'å…¶ä»–')
            else:
                raise ValueError("No JSON")
                
        except Exception as e:
            tweet['score'] = 50
            tweet['summary'] = tweet['content'][:80] + "..." if len(tweet['content']) > 80 else tweet['content']
            tweet['category'] = "å…¶ä»–"
        
        time.sleep(0.3)
    
    return tweets

def add_to_history(tweets):
    """æ·»åŠ åˆ°å†å²è®°å½•"""
    history = load_history()
    today = datetime.now().strftime("%Y-%m-%d")
    
    if today not in history:
        history[today] = []
    
    # å»é‡
    existing_links = {t['link'] for t in history[today]}
    new_tweets = [t for t in tweets if t['link'] not in existing_links]
    
    # æ·»åŠ åˆ°ä»Šæ—¥è®°å½•
    history[today].extend(new_tweets)
    
    # åªä¿ç•™å‰30æ¡
    history[today] = sorted(history[today], key=lambda x: x.get('score', 0), reverse=True)[:MAX_DAILY]
    
    # åªä¿ç•™æœ€è¿‘30å¤©
    cutoff_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
    history = {k: v for k, v in history.items() if k >= cutoff_date}
    
    save_history(history)
    return len(new_tweets)

def generate_appjs(top_tweets, history):
    """ç”Ÿæˆapp.js"""
    today = datetime.now().strftime("%Y-%m-%d")
    
    # ä»Šæ—¥æ˜¾ç¤ºï¼šTop10
    thoughts = []
    for t in top_tweets:
        thoughts.append({
            "content": f"[{t['category']}] {t['summary']}",
            "time": t['time'],
            "link": t['link'],
            "score": t['score'],
            "author": t['author']
        })
    
    # æ„å»ºarchiveï¼ˆå†å²å›é¡¾ï¼‰
    archive = []
    for date_str in sorted(history.keys(), reverse=True)[:7]:
        day_tweets = history[date_str]
        date_obj = datetime.strptime(date_str, "%Y-%m-%d")
        
        archive_item = {
            "date": date_obj.strftime("%Yå¹´%mæœˆ%dæ—¥"),
            "dateEn": date_obj.strftime("%B %d, %Y"),
            "rss": [],
            "ai": [],
            "thoughts": [{"content": f"[{t.get('category','å…¶ä»–')}] {t.get('summary', t['content'][:100])}", 
                         "time": t.get('time', ''), 
                         "link": t['link']} for t in day_tweets],
            "rec": []
        }
        archive.append(archive_item)
    
    now = datetime.now()
    content = {
        "date": now.strftime('%Yå¹´%mæœˆ%dæ—¥'),
        "dateEn": now.strftime('%B %d, %Y'),
        "rss": [],
        "ai": [],
        "thoughts": thoughts,
        "rec": [],
        "archive": archive
    }
    
    return f"const dailyContent = {json.dumps(content, indent=2, ensure_ascii=False)};"

def main():
    print(f"ğŸš€ Twitter Monitor | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ç›‘æ§ {len(TWITTER_ACCOUNTS)} ä¸ªè´¦å·")
    print("-" * 60)
    
    # åŠ è½½å†å²
    history = load_history()
    
    # æŠ“å–æ‰€æœ‰è´¦å·
    all_tweets = []
    for i, account in enumerate(TWITTER_ACCOUNTS):
        print(f"[{i+1}/{len(TWITTER_ACCOUNTS)}] @{account}...", end=" ")
        tweets = fetch_tweets(account, i)
        print(f"{len(tweets)}æ¡")
        all_tweets.extend(tweets)
        time.sleep(1)
    
    print(f"\nğŸ“Š å…±è·å– {len(all_tweets)} æ¡æ–°æ¨æ–‡")
    
    if not all_tweets:
        print("â­ï¸ æ²¡æœ‰æ–°æ¨æ–‡")
        top_tweets = []
    else:
        # AIåˆ†æ
        print("\nğŸ¤– DeepSeekåˆ†æä¸­...")
        scored_tweets = ai_score_and_summarize(all_tweets)
        
        # æ·»åŠ åˆ°å†å²
        added_count = add_to_history(scored_tweets)
        print(f"ğŸ“ æ–°å¢ {added_count} æ¡åˆ°å†å²è®°å½•")
        
        # å–Top10æ˜¾ç¤º
        top_tweets = sorted(scored_tweets, key=lambda x: x.get('score', 0), reverse=True)[:10]
        
        print(f"\nğŸ† Top 10:")
        for i, t in enumerate(top_tweets, 1):
            print(f"  {i}. [{t['score']}åˆ†] @{t['author']}: {t['summary'][:40]}...")
    
    # é‡æ–°åŠ è½½å†å²
    history = load_history()
    
    # ç”Ÿæˆapp.js
    print("\nğŸ“ ç”Ÿæˆ app.js...")
    new_content = generate_appjs(top_tweets, history)
    
    output_path = "daily-digest/app.js"
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    print(f"âœ… å·²ä¿å­˜åˆ° {output_path}")
    
    # æ ‡è®°æ›´æ–°
    with open("content_updated.flag", 'w') as f:
        f.write(f"updated_at_{datetime.now().isoformat()}")
    
    # ç»Ÿè®¡
    today = datetime.now().strftime("%Y-%m-%d")
    today_count = len(history.get(today, []))
    print(f"\nğŸ“ˆ ä»Šæ—¥å·²æ”¶é›†: {today_count}/{MAX_DAILY} æ¡")
    print(f"âœ¨ ä¸‹æ¬¡æ£€æŸ¥: {(datetime.now() + timedelta(minutes=5)).strftime('%H:%M')}")

if __name__ == "__main__":
    main()
